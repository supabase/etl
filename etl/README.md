# `etl` - Core

This is the main crate of the ETL system, providing the core functionality for PostgreSQL logical replication. It abstracts the complexities of PostgreSQL's logical streaming replication protocol and provides a unified interface for data replication and transformation.

## Features

| Feature                  | Description                                |
| ------------------------ | ------------------------------------------ |
| `unknown-types-to-bytes` | Converts unknown PostgreSQL types to bytes (enabled by default) |
| `test-utils`             | Enables testing utilities and helpers      |
| `failpoints`             | Enables failure injection for testing      |

## Architecture

The ETL core implements a pipeline architecture that replicates data from PostgreSQL to various destinations.

### Key Components

- **Pipeline**: Main orchestrator that manages the replication process
- **Replication Client**: Connects to PostgreSQL's logical replication protocol
- **Apply Worker**: Main worker that handles the creation of table sync workers and processes CDC events
- **Table Sync Worker**: Handles initial copying of existing table data and processes CDC events until it has caught up
  to the apply worker
- **State Store**: Tracks the state of the pipeline
- **Schema Store**: Tracks the table schemas of the tables involved in the replication

### Information Flow

```mermaid
graph TB    
    subgraph "ETL Pipeline"
        Pipeline["ğŸ­ Pipeline"]
        
        ApplyWorker["âš™ï¸ Apply Worker"]

        subgraph "Worker Pool"
            TSWorker1["ğŸ”„ Table Sync Worker 1"]
            TSWorkerN["ğŸ”„ Table Sync Worker N"]
        end
        
        subgraph "Store"
            StateStore["ğŸ’¾ State Store"]
            SchemaStore["ğŸ“‹ Schema Store"]
        end
    end

    PG[("ğŸ˜ PostgreSQL<br/>Source Database")]
    
    Destination[("ğŸ¯ Destination<br/>BigQuery, etc.")]
    
    Pipeline --> ApplyWorker
    
    ApplyWorker --> TSWorker1
    ApplyWorker --> TSWorkerN
    
    ApplyWorker --> Destination
    TSWorker1 --> Destination
    TSWorkerN --> Destination

    ApplyWorker <--> PG
    TSWorker1 <--> PG

    ApplyWorker <--> StateStore
    ApplyWorker <--> SchemaStore

    TSWorker1 <--> StateStore
    TSWorker1 <--> SchemaStore

    TSWorker2 <--> StateStore
    TSWorker2 <--> SchemaStore
```

### How It Works

1. **Pipeline** orchestrates the entire replication process
2. **Apply Worker** processes CDC events from the replication stream and spawns table sync workers as needed
3. **Table Sync Workers** are created in a pool to handle initial table copying independently
4. Each worker **polls PostgreSQL independently** - Apply Worker reads CDC stream, Table Sync Workers copy table data
5. All workers **write independently to destinations** for optimal throughput

